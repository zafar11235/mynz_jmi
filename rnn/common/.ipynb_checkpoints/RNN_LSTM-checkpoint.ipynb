{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our RNN-LSTM Model for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "1. [Beam search Keras code](https://gist.github.com/udibr/67be473cf053d8c38730)\n",
    "2. [Keras next character prediction](https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py)\n",
    "3. [Skip Thoughts Paper](http://arxiv.org/pdf/1506.06726v1.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Variables to be used later\n",
    "vocabulary_size = 2**13\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "file_name = 'reddit-comments-2015-08.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text is 7991963 characters long\n"
     ]
    }
   ],
   "source": [
    "# Read csv file and append sentences into text after adding delimiters\n",
    "text = \"\"\n",
    "with open(file_name) as f:\n",
    "    reader = csv.reader(f,skipinitialspace=True)\n",
    "    for x in reader:\n",
    "        text += \"%s %s %s \" %(sentence_start_token,x[0],sentence_end_token)\n",
    "    \n",
    "print \"The text is %d characters long\" %len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "printable = set(string.printable)\n",
    "text = filter(lambda x: x in printable, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1587838 tokens generated\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = nltk.word_tokenize(text)\n",
    "print '%d tokens generated' %(len(tokenized_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1584481\n"
     ]
    }
   ],
   "source": [
    "# Remove long strings\n",
    "temp = [s for s in tokenized_text if len(s) <= 30]\n",
    "tokenized_text = temp\n",
    "print len(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 73292 unique word tokens\n"
     ]
    }
   ],
   "source": [
    "## Counting the word frequencies in the word_tokenize\n",
    "word_freq = nltk.FreqDist(tokenized_text)\n",
    "print \"Found %d unique word tokens\"%(len(word_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vocabulary size 8192.\n",
      "The least frequent word in our vocabulary is 'consist' and appeared 10 times\n"
     ]
    }
   ],
   "source": [
    "## Hashing the most frequent words into the vocabulary\n",
    "vocab = word_freq.most_common(vocabulary_size - 1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([w,i] for i,w in enumerate(index_to_word))\n",
    "print \"Using vocabulary size %d.\" % vocabulary_size\n",
    "print \"The least frequent word in our vocabulary is '%s' and appeared %d times\"%(vocab[-1][0],vocab[-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, word in enumerate(tokenized_text):\n",
    "    if word not in word_to_index:\n",
    "        tokenized_text[i] = unknown_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('nb sequences:', 528157)\n"
     ]
    }
   ],
   "source": [
    "maxlen = 10\n",
    "step = 3\n",
    "sentences = np.empty([len(range(0, len(tokenized_text) - maxlen, step)), maxlen], dtype='a30')\n",
    "next_word = np.empty([len(range(0, len(tokenized_text) - maxlen, step)), 1], dtype ='a30')\n",
    "\n",
    "count = 0\n",
    "for i in range(0, len(tokenized_text) - maxlen, step):\n",
    "    sentences[count] = tokenized_text[i: i + maxlen]\n",
    "    next_word[count] = tokenized_text[i + maxlen]\n",
    "    count += 1\n",
    "print('nb sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('UNKNOWN_TOKEN', 43554),\n",
       " ('.', 22820),\n",
       " (',', 17359),\n",
       " ('the', 16197),\n",
       " ('to', 11584),\n",
       " ('a', 10436),\n",
       " ('I', 10073),\n",
       " ('and', 9497),\n",
       " ('of', 7617),\n",
       " ('you', 6573)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = next_word.tolist()\n",
    "n = [l[0] for l in n]\n",
    "from collections import Counter\n",
    "count = Counter(n)\n",
    "count.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since we have memory issue, we will free variables that will not be used now\n",
    "import gc\n",
    "del text, tokenized_text, word_freq, vocab\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X_train and y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating a small dataset out of our given sentences\n",
    "# We will train in batches\n",
    "subdata_size = 32768\n",
    "sents = sentences[subdata_size:2*subdata_size]\n",
    "nexts = next_word[subdata_size:2*subdata_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "print('Vectorization...')\n",
    "X = np.zeros((len(sents), maxlen), dtype='int_')\n",
    "y = np.zeros((len(sents), vocabulary_size), dtype=np.bool)\n",
    "for i, sentence in enumerate(sents):\n",
    "    for t, word in enumerate(sentence):\n",
    "        X[i, t] = word_to_index[word]\n",
    "    y[i, word_to_index[nexts[i][0]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32768, 10), (32768, 8192))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Saving the numpy array for future\n",
    "# np.savez('x_train', X)\n",
    "# np.savez('y_train', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense,TimeDistributed, Dense, Dropout\n",
    "\n",
    "EMBED_HIDDEN_SIZE = 512\n",
    "LSTM_SIZE = 512\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, EMBED_HIDDEN_SIZE, input_length=maxlen, name='Embedding'))\n",
    "model.add(Dropout(0.3, name='Dropout1'))\n",
    "model.add(LSTM(EMBED_HIDDEN_SIZE, return_sequences=True, input_shape=(maxlen, EMBED_HIDDEN_SIZE), name='LSTM1'))\n",
    "model.add(Dropout(0.2, name='Dropout2'))\n",
    "model.add(LSTM(512, return_sequences=False, name='LSTM2'))\n",
    "model.add(Dropout(0.2, name='Dropout3'))\n",
    "model.add(Dense(vocabulary_size, activation='softmax', name='Dense'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                       Output Shape        Param #     Connected to                     \n",
      "====================================================================================================\n",
      "Embedding (Embedding)              (None, 10, 512)     4194304     embedding_input_7[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "Dropout1 (Dropout)                 (None, 10, 512)     0           Embedding[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "LSTM1 (LSTM)                       (None, 10, 512)     2099200     Dropout1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "Dropout2 (Dropout)                 (None, 10, 512)     0           LSTM1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "LSTM2 (LSTM)                       (None, 512)         2099200     Dropout2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "Dropout3 (Dropout)                 (None, 512)         0           LSTM2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "Dense (Dense)                      (None, 8192)        4202496     Dropout3[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 12595200\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "32768/32768 [==============================] - 37s - loss: 9.2772 - acc: 0.1064    \n",
      "Epoch 2/50\n",
      "32768/32768 [==============================] - 37s - loss: 6.1552 - acc: 0.1263    \n",
      "Epoch 3/50\n",
      "32768/32768 [==============================] - 38s - loss: 5.5169 - acc: 0.1544    \n",
      "Epoch 4/50\n",
      "32768/32768 [==============================] - 38s - loss: 5.2634 - acc: 0.1660    \n",
      "Epoch 5/50\n",
      "32768/32768 [==============================] - 38s - loss: 5.0826 - acc: 0.1760    \n",
      "Epoch 6/50\n",
      "32768/32768 [==============================] - 38s - loss: 4.9142 - acc: 0.1843    \n",
      "Epoch 7/50\n",
      "32768/32768 [==============================] - 38s - loss: 4.7549 - acc: 0.1938    \n",
      "Epoch 8/50\n",
      "32768/32768 [==============================] - 38s - loss: 4.5955 - acc: 0.2041    \n",
      "Epoch 9/50\n",
      "32768/32768 [==============================] - 38s - loss: 4.4315 - acc: 0.2115    \n",
      "Epoch 10/50\n",
      "32768/32768 [==============================] - 39s - loss: 4.2644 - acc: 0.2252    \n",
      "Epoch 11/50\n",
      "32768/32768 [==============================] - 38s - loss: 4.0789 - acc: 0.2376    \n",
      "Epoch 12/50\n",
      "32768/32768 [==============================] - 38s - loss: 3.8952 - acc: 0.2542    \n",
      "Epoch 13/50\n",
      "32768/32768 [==============================] - 38s - loss: 3.7087 - acc: 0.2688    \n",
      "Epoch 14/50\n",
      "32768/32768 [==============================] - 38s - loss: 3.5021 - acc: 0.2911    \n",
      "Epoch 15/50\n",
      "32768/32768 [==============================] - 38s - loss: 3.2987 - acc: 0.3150    \n",
      "Epoch 16/50\n",
      "32768/32768 [==============================] - 38s - loss: 3.0738 - acc: 0.3434    \n",
      "Epoch 17/50\n",
      "32768/32768 [==============================] - 38s - loss: 2.8681 - acc: 0.3746    \n",
      "Epoch 18/50\n",
      "32768/32768 [==============================] - 38s - loss: 2.6601 - acc: 0.4100    \n",
      "Epoch 19/50\n",
      "32768/32768 [==============================] - 38s - loss: 2.4699 - acc: 0.4400    \n",
      "Epoch 20/50\n",
      "32768/32768 [==============================] - 38s - loss: 2.2866 - acc: 0.4784    \n",
      "Epoch 21/50\n",
      "32768/32768 [==============================] - 38s - loss: 2.1036 - acc: 0.5126    \n",
      "Epoch 22/50\n",
      "32768/32768 [==============================] - 38s - loss: 1.9467 - acc: 0.5466    \n",
      "Epoch 23/50\n",
      "32768/32768 [==============================] - 39s - loss: 1.7802 - acc: 0.5807    \n",
      "Epoch 24/50\n",
      "32768/32768 [==============================] - 38s - loss: 1.6494 - acc: 0.6109    \n",
      "Epoch 25/50\n",
      "32768/32768 [==============================] - 38s - loss: 1.5317 - acc: 0.6358    \n",
      "Epoch 26/50\n",
      "32768/32768 [==============================] - 39s - loss: 1.4091 - acc: 0.6613    \n",
      "Epoch 27/50\n",
      "32768/32768 [==============================] - 39s - loss: 1.3039 - acc: 0.6882    \n",
      "Epoch 28/50\n",
      "32768/32768 [==============================] - 39s - loss: 1.2113 - acc: 0.7083    \n",
      "Epoch 29/50\n",
      "32768/32768 [==============================] - 39s - loss: 1.1233 - acc: 0.7289    \n",
      "Epoch 30/50\n",
      "32768/32768 [==============================] - 39s - loss: 1.0393 - acc: 0.7478    \n",
      "Epoch 31/50\n",
      "32768/32768 [==============================] - 39s - loss: 0.9652 - acc: 0.7692    \n",
      "Epoch 32/50\n",
      "32768/32768 [==============================] - 39s - loss: 0.9029 - acc: 0.7850    \n",
      "Epoch 33/50\n",
      "32768/32768 [==============================] - 39s - loss: 0.8412 - acc: 0.7968    \n",
      "Epoch 34/50\n",
      "32768/32768 [==============================] - 39s - loss: 0.7834 - acc: 0.8127    \n",
      "Epoch 35/50\n",
      "32768/32768 [==============================] - 39s - loss: 0.7242 - acc: 0.8293    \n",
      "Epoch 36/50\n",
      "32768/32768 [==============================] - 39s - loss: 0.6877 - acc: 0.8376    \n",
      "Epoch 37/50\n",
      "32768/32768 [==============================] - 39s - loss: 0.6461 - acc: 0.8466    \n",
      "Epoch 38/50\n",
      "32768/32768 [==============================] - 39s - loss: 0.6121 - acc: 0.8557    \n",
      "Epoch 39/50\n",
      "32768/32768 [==============================] - 39s - loss: 0.5860 - acc: 0.8638    \n",
      "Epoch 40/50\n",
      "32768/32768 [==============================] - 39s - loss: 0.5540 - acc: 0.8701    \n",
      "Epoch 41/50\n",
      "32768/32768 [==============================] - 39s - loss: 0.5221 - acc: 0.8804    \n",
      "Epoch 42/50\n",
      "32768/32768 [==============================] - 38s - loss: 0.4895 - acc: 0.8874    \n",
      "Epoch 43/50\n",
      "32768/32768 [==============================] - 40s - loss: 0.4729 - acc: 0.8924    \n",
      "Epoch 44/50\n",
      "32768/32768 [==============================] - 36s - loss: 0.4565 - acc: 0.8976    \n",
      "Epoch 45/50\n",
      "32768/32768 [==============================] - 37s - loss: 0.4352 - acc: 0.9030    \n",
      "Epoch 46/50\n",
      "32768/32768 [==============================] - 39s - loss: 0.4154 - acc: 0.9077    \n",
      "Epoch 47/50\n",
      "32768/32768 [==============================] - 40s - loss: 0.4070 - acc: 0.9080    \n",
      "Epoch 48/50\n",
      "32768/32768 [==============================] - 40s - loss: 0.4013 - acc: 0.9111    \n",
      "Epoch 49/50\n",
      "32768/32768 [==============================] - 38s - loss: 0.3744 - acc: 0.9182    \n",
      "Epoch 50/50\n",
      "32768/32768 [==============================] - 39s - loss: 0.3645 - acc: 0.9201    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f614fcfe550>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, batch_size=128, nb_epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save_weights('/home/najeeb/Desktop/Dataset/mynz_jmi/zafar.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('/home/najeeb/Desktop/Dataset/mynz_jmi/weights_final.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "the\n",
      "30\n",
      "him\n",
      "her\n",
      "maybe\n",
      "and\n",
      "doing\n",
      "your\n",
      "up\n",
      "his\n"
     ]
    }
   ],
   "source": [
    "sentence = 'SENTENCE_START While on our way to the show we saw'\n",
    "x = nltk.word_tokenize(sentence)\n",
    "a = [word_to_index[i] for i in x] \n",
    "x_predict = np.array([a])\n",
    "y = model.predict_proba(x_predict)\n",
    "top_10 = np.argsort(y[0])[::-1][:10]\n",
    "for i in top_10:\n",
    "    print index_to_word[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Prediction Using the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Function to convert a sentence in a matrix\n",
    "def sentence_lemmatized(sentence):\n",
    "    X = nltk.word_tokenize(sentence)\n",
    "    A = [word_to_index[w] if w in word_to_index else word_to_index[unknown_token] for w in X] \n",
    "    X_predict = np.array([A])\n",
    "    return X_predict\n",
    "## Function to predict the next word\n",
    "def predict_word(X):\n",
    "    yProb = model.predict_proba(X,verbose = 0)\n",
    "    top = np.argsort(yProb[0])[::-1][:20]\n",
    "    if index_to_word[top[0]] == unknown_token:\n",
    "        return index_to_word[top[1]]\n",
    "    else:\n",
    "        return index_to_word[top[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Generating words so as to form a sentence\n",
    "def form_text(sentence,para_length):\n",
    "    print \"Forming text(%d words)\"%(para_length)\n",
    "    print \"--------------------------------------------------\"\n",
    "    i,flag,text = 0,0,[]\n",
    "    while (i < para_length):\n",
    "        s = sentence_lemmatized(sentence)\n",
    "        word = predict_word(s)\n",
    "        if flag == 1:\n",
    "            text.append(word)\n",
    "            i += 1\n",
    "        if word == '.' or word == sentence_start_token:\n",
    "            flag = 1\n",
    "        sentence = sentence_start_token + ' ' + ' '.join(sentence.split(' ')[2:]) + \" \" + word\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_paragraphs(n_texts,sentence,para_length):\n",
    "    for i in range(n_texts):\n",
    "        string = form_text(sentence,para_length)\n",
    "        print \"Paragraph Number %d\"%(i+1)\n",
    "        print \"--------------------------------------------------\"\n",
    "        print string\n",
    "        print \"--------------------------------------------------\"\n",
    "        sentence = sentence_start_token+\" \"+\" \".join(string.split(' ')[0:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forming text(100 words)\n",
      "--------------------------------------------------\n",
      "Paragraph Number 1\n",
      "--------------------------------------------------\n",
      "So he 'd always make to buy running ! I thought my faith less being able to deal with any things that he was telling him the OP the last different language . But since the government is not an OP ) . You 're trying to say that the only comment very similar wall , and I do hope I 'd be able to do a lot of people who seem help sense who would be some other research . I have been removed because you have being helped ? SENTENCE_END SENTENCE_START Your post has been removed because its\n",
      "--------------------------------------------------\n",
      "Forming text(100 words)\n",
      "--------------------------------------------------\n",
      "Paragraph Number 2\n",
      "--------------------------------------------------\n",
      "But since the government is not an OP ) . You 're trying to say that the only comment very similar wall , and I do hope I 'd be able to do a lot of people who seem help sense who would be some other research . I have been removed because you have being helped ? SENTENCE_END SENTENCE_START Your post has been removed because its a lot of intelligence n't need to read an relationship that you need to buy me . It 's not really all the entire hardware upgrades , but you do n't know what\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "sentence = \"SENTENCE_START While on our way to the show we saw\"\n",
    "generate_paragraphs(2,sentence,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
