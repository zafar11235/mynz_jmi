{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing LSTM Based Next Word Prediction Using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 15001 sentences\n"
     ]
    }
   ],
   "source": [
    "maxlen = 10\n",
    "no_word = 'NO_WORD'\n",
    "vocabulary_size = 10000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "with open('reddit-comments-2015-08.csv') as f:\n",
    "    reader = csv.reader(f,skipinitialspace=True)\n",
    "    ## Split comments into sentences\n",
    "    sentences  = itertools.chain(*[nltk.sent_tokenize(x[0].decode('utf-8').lower()) for x in reader])\n",
    "    sentences = [\"%s %s %s\"%(sentence_start_token,x,sentence_end_token) for x in reader]\n",
    "print \"Parsed %d sentences\"%(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1716192 number of words\n"
     ]
    }
   ],
   "source": [
    "## Tokenize the sentences into words\n",
    "count = 0\n",
    "tokenized_words = [nltk.word_tokenize(word) for word in sentences]\n",
    "for l in tokenized_words:\n",
    "    count += len(l)\n",
    "print \"Found %d number of words\"%(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 65751 unique word tokens\n"
     ]
    }
   ],
   "source": [
    "## Counting the word frequencies in the word_tokenize\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_words))\n",
    "print \"Found %d unique word tokens\"%(len(word_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vocabulary size 10000.\n",
      "The least frequent word in our vocabulary is 'warnings' and appeared 7 times\n"
     ]
    }
   ],
   "source": [
    "## Hashing the most frequent words into the vocabulary\n",
    "vocab = word_freq.most_common(vocabulary_size - 1)\n",
    "vocab.insert(0,(u'NO_WORD',1000))\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([w,i] for i,w in enumerate(index_to_word))\n",
    "print \"Using vocabulary size %d.\" % vocabulary_size\n",
    "print \"The least frequent word in our vocabulary is '%s' and appeared %d times\"%(vocab[-1][0],vocab[-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_words):\n",
    "    tokenized_words[i] = [w if w in word_to_index else unknown_token for w in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Creating each sentence in the collection to be 10 words long\n",
    "## I have split the sentences unknowingly thus making sentence incomplete\n",
    "## This methos needs to be checked\n",
    "text,next_word,maxlen = [],[],\n",
    "for sent in tokenized_words:\n",
    "    if len(sent) < maxlen:\n",
    "        continue\n",
    "    if len(sent) >= maxlen:\n",
    "        val = sent[0:maxlen-1]\n",
    "        val.append(sentence_end_token)\n",
    "        text.append(val)\n",
    "        next_word.append(sent[maxlen-1])\n",
    "print text[0:10]\n",
    "print \"--------------------\"\n",
    "print next_word[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Vectorizing each of the sentence into the matrix X\n",
    "## Matrix y contains the next word prediction for the whole sentence (LSTM)\n",
    "X = np.zeros((len(text),maxlen,vocabulary_size),dtype = np.bool)\n",
    "y = np.zeros((len(text),vocabulary_size),dtype = np.bool)\n",
    "for i,sent in enumerate(text):\n",
    "    for t,word in enumerate(sent):\n",
    "        X[i,t,word_to_index[word]] = 1\n",
    "    y[i,word_to_index[next_word[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X[0:1]\n",
    "print \"---------------------------------------------------\"\n",
    "print y[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(512, return_sequences=True, input_shape=(maxlen,vocabulary_size)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(vocabulary_size))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(X, y, batch_size=128, nb_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Model needs to be trained on a GPU\n",
    "## The output needs to be predicted by using a mixture of sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Using Masking Rather Than To Slice The Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "next_word = []\n",
    "def mask(string):\n",
    "    if len(string) < maxlen:\n",
    "        ## Pad the string with no_word type of string\n",
    "        next_word.append(string[-2])\n",
    "        l = [no_word for i in range(maxlen - len(string) + 1)]\n",
    "        string.remove(sentence_end_token)\n",
    "        string[-1] = sentence_end_token\n",
    "        string = l + string\n",
    "    else:\n",
    "        string = string[0:maxlen]\n",
    "        next_word.append(string.pop(maxlen-1))\n",
    "        string.append(sentence_end_token)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,sent in enumerate(tokenized_words):\n",
    "    if len(sent) <= 3:\n",
    "        tokenized_words.pop(i)\n",
    "for i,sent in enumerate(tokenized_words):\n",
    "    tokenized_words[i] = mask(tokenized_words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print tokenized_words[0:10]\n",
    "print '--------------------------------------------'\n",
    "print next_word[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Frequency distribution of the next word\n",
    "dist = nltk.FreqDist(next_word)\n",
    "for key in sorted(dist,key=dist.get,reverse=True)[0:10]:\n",
    "    print str(key.encode('utf-8')) + \" : \" + str(dist[unicode(key)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the sliding window approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## The above model seem to have not captured the essence of long words\n",
    "## So we use the sliding window approach\n",
    "## Converting the segregated sentences into one sentence (text)\n",
    "text = []\n",
    "for string in tokenized_words:\n",
    "    for word in string:\n",
    "        text.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('nb sequences:', 572061)\n"
     ]
    }
   ],
   "source": [
    "## Making a sentence matrix having sentences of length 'maxlen'\n",
    "sent,next_word = [],[]\n",
    "step = 3\n",
    "for i in range(0,len(text) - maxlen,step):\n",
    "    sent.append(text[i:i+maxlen])\n",
    "    next_word.append(text[i+maxlen])\n",
    "print('nb sequences:', len(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('UNKNOWN_TOKEN', 32235),\n",
       " (u'SENTENCE_START', 26491),\n",
       " (u'SENTENCE_END', 26394),\n",
       " (u'.', 22495),\n",
       " (u'the', 17454),\n",
       " (u',', 17378),\n",
       " (u'to', 11888),\n",
       " (u'i', 10660),\n",
       " (u'a', 10526),\n",
       " (u'and', 10046)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(next_word).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Sanity Check\n",
    "for string in sent:\n",
    "    if len(string) != 10:\n",
    "        print \"Warning\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Memory is constrained so dropping some items\n",
    "import gc\n",
    "del tokenized_words,text,word_freq,vocab,sentences\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Due to constraints on the Memory we will use only a subset of our data\n",
    "data_size = 30000\n",
    "sent = sent[data_size:2*data_size]\n",
    "next_word = next_word[data_size:2*data_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Vectorizing each of the sentence into the matrix X\n",
    "## Matrix y contains the next word prediction for the whole sentence (LSTM)\n",
    "X = np.zeros((len(sent),maxlen,vocabulary_size+1),dtype = np.bool)\n",
    "y = np.zeros((len(sent),vocabulary_size+1),dtype = np.bool)\n",
    "for i,s in enumerate(sent):\n",
    "    for t,word in enumerate(s):\n",
    "        X[i,t,word_to_index[word]] = 1\n",
    "    y[i,word_to_index[next_word[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  ..., \n",
      "  [False False  True ..., False False False]\n",
      "  [False  True False ..., False False False]\n",
      "  [False False False ..., False False False]]]\n",
      "---------------------------------------------------\n",
      "[[False False False ..., False False False]]\n"
     ]
    }
   ],
   "source": [
    "print X[0:1]\n",
    "print \"---------------------------------------------------\"\n",
    "print y[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 10, 10001)\n",
      "(30000, 10001)\n"
     ]
    }
   ],
   "source": [
    "print np.shape(X)\n",
    "print np.shape(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(512, return_sequences=True, input_shape=(maxlen,vocabulary_size+1)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(vocabulary_size+1))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                       Output Shape        Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lstm_1 (LSTM)                      (None, 10, 512)     21532672    lstm_input_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)                (None, 10, 512)     0           lstm_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                      (None, 512)         2099200     dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)                (None, 512)         0           lstm_2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                    (None, 10001)       5130513     dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)          (None, 10001)       0           dense_1[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 28762385\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adagrad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.load_weights('weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(X, y, batch_size=128, nb_epoch=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save_weights('weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 10000, 11, 125, 63, 25, 122, 6, 4, 2]\n"
     ]
    }
   ],
   "source": [
    "string = 'Are you going out with him to the'\n",
    "def convert(string):\n",
    "    word = nltk.word_tokenize(string)\n",
    "    word.insert(0,sentence_start_token)\n",
    "    word.append(sentence_end_token)\n",
    "    word = [word_to_index[w] if w in word_to_index else word_to_index[unknown_token] for w in word]\n",
    "    return word\n",
    "print convert(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[False  True False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  ..., \n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]]\n",
      "\n",
      " [[False False False ..., False False False]\n",
      "  [False False False ..., False False  True]\n",
      "  [False False False ..., False False False]\n",
      "  ..., \n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]]\n",
      "\n",
      " [[False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  ..., \n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]]\n",
      "\n",
      " ..., \n",
      " [[False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  ..., \n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]]\n",
      "\n",
      " [[False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  ..., \n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]]\n",
      "\n",
      " [[False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  ..., \n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False  True ..., False False False]]]\n"
     ]
    }
   ],
   "source": [
    "word,i = convert(string),0\n",
    "X_test = np.zeros((len(word),maxlen,vocabulary_size+1),dtype = np.bool)\n",
    "for t,w in enumerate(word):\n",
    "    X_test[i,t,w] = 1\n",
    "    i += 1\n",
    "print X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s\n",
      "10/10 [==============================] - 0s\n",
      "10/10 [==============================] - 0s\n",
      "10/10 [==============================] - 0s\n",
      "10/10 [==============================] - 0s\n",
      "10/10 [==============================] - 0s\n",
      "10/10 [==============================] - 0s\n",
      "10/10 [==============================] - 0s\n",
      "10/10 [==============================] - 0s\n",
      "10/10 [==============================] - 0s\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    yPred = model.predict_classes(X_test,verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10000 10000 10000 10000 10000 10000 10000 10000 10000 10000]\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "print yPred\n",
    "print yPred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNKNOWN_TOKEN\n",
      "UNKNOWN_TOKEN\n",
      "UNKNOWN_TOKEN\n",
      "UNKNOWN_TOKEN\n",
      "UNKNOWN_TOKEN\n",
      "UNKNOWN_TOKEN\n",
      "UNKNOWN_TOKEN\n",
      "UNKNOWN_TOKEN\n",
      "UNKNOWN_TOKEN\n",
      "UNKNOWN_TOKEN\n"
     ]
    }
   ],
   "source": [
    "for word in yPred:\n",
    "    print index_to_word[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
